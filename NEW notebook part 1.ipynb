{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wildalaska/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Import Python libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import preprocessing \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from pylab import *\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>about me</th>\n",
       "      <th>doingwithlife</th>\n",
       "      <th>strengths</th>\n",
       "      <th>whatothersnoticefirst</th>\n",
       "      <th>mediainterests</th>\n",
       "      <th>cantdowithout</th>\n",
       "      <th>oftenthinkabout</th>\n",
       "      <th>usualfriday</th>\n",
       "      <th>secret</th>\n",
       "      <th>messagemeif</th>\n",
       "      <th>think_revised</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>thinknopunc</th>\n",
       "      <th>char_count</th>\n",
       "      <th>thinkredux</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>think_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>about me:  i would love to think that i was so...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh. ranting about a good salt...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>books: absurdistan, the republic, of mice and ...</td>\n",
       "      <td>food. water. cell phone. shelter.</td>\n",
       "      <td>duality and humorous things</td>\n",
       "      <td>trying to find someone to hang out with. i am ...</td>\n",
       "      <td>i am new to california and looking for someone...</td>\n",
       "      <td>you want to be swept off your feet! you are ti...</td>\n",
       "      <td>duality humorous things</td>\n",
       "      <td>1</td>\n",
       "      <td>duality humorous things</td>\n",
       "      <td>23</td>\n",
       "      <td>duality humorous things</td>\n",
       "      <td>['duality', 'humorous', 'thing']</td>\n",
       "      <td>['duality', 'humorous', 'thing']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>m</td>\n",
       "      <td>i'm an australian living in san francisco, but...</td>\n",
       "      <td>building awesome stuff. figuring out what's im...</td>\n",
       "      <td>imagining random shit. laughing at aforementio...</td>\n",
       "      <td>i have a big smile. i also get asked if i'm we...</td>\n",
       "      <td>books: to kill a mockingbird, lord of the ring...</td>\n",
       "      <td>like everyone else, i love my friends and fami...</td>\n",
       "      <td>what my contribution to the world is going to ...</td>\n",
       "      <td>out with my friends!</td>\n",
       "      <td>i cried on my first day at school because a bi...</td>\n",
       "      <td>you're awesome.</td>\n",
       "      <td>contribution world going and/or be. what's bre...</td>\n",
       "      <td>11</td>\n",
       "      <td>contribution world going andor be whats breakf...</td>\n",
       "      <td>64</td>\n",
       "      <td>contribution world going andor be whats breakf...</td>\n",
       "      <td>['contribution', 'world', 'going', 'andor', 'b...</td>\n",
       "      <td>['contribution', 'world', 'going', 'andor', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>m</td>\n",
       "      <td>my names jake. i'm a creative guy and i look f...</td>\n",
       "      <td>i have an apartment. i like to explore and che...</td>\n",
       "      <td>i'm good at finding creative solutions to prob...</td>\n",
       "      <td>i'm short</td>\n",
       "      <td>i like some tv. i love summer heights high and...</td>\n",
       "      <td>music, my guitar contrast good food my bike my...</td>\n",
       "      <td>you should</td>\n",
       "      <td>send a message</td>\n",
       "      <td>and say hi.</td>\n",
       "      <td>you can rock the bells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>m</td>\n",
       "      <td>update: i'm seeing someone, so off the market ...</td>\n",
       "      <td>i have three jobs. i've been doing sound and l...</td>\n",
       "      <td>hugging, kissing, laughing, motivating people,...</td>\n",
       "      <td>my huge goofy smile</td>\n",
       "      <td>i'm constantly reading, i read at what my frie...</td>\n",
       "      <td>family friends food women music reading</td>\n",
       "      <td>snowboarding, food, women, goofy nerd stuff, a...</td>\n",
       "      <td>having dinner and drinks with friends and/or w...</td>\n",
       "      <td>i used to wish for a jetpack when blowing out ...</td>\n",
       "      <td>you are a complex woman with healthy self-este...</td>\n",
       "      <td>snowboarding, food, women, goofy nerd stuff, a...</td>\n",
       "      <td>0</td>\n",
       "      <td>snowboarding food women goofy nerd stuff archi...</td>\n",
       "      <td>59</td>\n",
       "      <td>snowboarding food women goofy nerd stuff archi...</td>\n",
       "      <td>['snowboarding', 'food', 'woman', 'goofy', 'ne...</td>\n",
       "      <td>['snowboarding', 'food', 'woman', 'goofy', 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>m</td>\n",
       "      <td>i was born in wisconsin, grew up in iowa, and ...</td>\n",
       "      <td>i'm currently the youngest member on an intern...</td>\n",
       "      <td>i'm really good at a little bit of everything....</td>\n",
       "      <td>the way i dress. some days it's hats, other da...</td>\n",
       "      <td>books = yes. avid reader. moves = eternal suns...</td>\n",
       "      <td>guitar - even if i don't play it all the time,...</td>\n",
       "      <td>a little bit of everything. but mostly social ...</td>\n",
       "      <td>hanging out with a small group of friends--sta...</td>\n",
       "      <td>i'm picky when it comes to dating. i know what...</td>\n",
       "      <td>if you know who you are, who you want, where y...</td>\n",
       "      <td>little bit everything. mostly social influence...</td>\n",
       "      <td>16</td>\n",
       "      <td>little bit everything mostly social influences...</td>\n",
       "      <td>170</td>\n",
       "      <td>little bit everything mostly social influences...</td>\n",
       "      <td>['little', 'bit', 'everything', 'mostly', 'soc...</td>\n",
       "      <td>['little', 'bit', 'everything', 'mostly', 'soc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age sex                                           about me  \\\n",
       "0           0   22   m  about me:  i would love to think that i was so...   \n",
       "1           5   29   m  i'm an australian living in san francisco, but...   \n",
       "2           9   37   m  my names jake. i'm a creative guy and i look f...   \n",
       "3          10   35   m  update: i'm seeing someone, so off the market ...   \n",
       "4          11   28   m  i was born in wisconsin, grew up in iowa, and ...   \n",
       "\n",
       "                                       doingwithlife  \\\n",
       "0  currently working as an international agent fo...   \n",
       "1  building awesome stuff. figuring out what's im...   \n",
       "2  i have an apartment. i like to explore and che...   \n",
       "3  i have three jobs. i've been doing sound and l...   \n",
       "4  i'm currently the youngest member on an intern...   \n",
       "\n",
       "                                           strengths  \\\n",
       "0  making people laugh. ranting about a good salt...   \n",
       "1  imagining random shit. laughing at aforementio...   \n",
       "2  i'm good at finding creative solutions to prob...   \n",
       "3  hugging, kissing, laughing, motivating people,...   \n",
       "4  i'm really good at a little bit of everything....   \n",
       "\n",
       "                               whatothersnoticefirst  \\\n",
       "0  the way i look. i am a six foot half asian, ha...   \n",
       "1  i have a big smile. i also get asked if i'm we...   \n",
       "2                                          i'm short   \n",
       "3                                my huge goofy smile   \n",
       "4  the way i dress. some days it's hats, other da...   \n",
       "\n",
       "                                      mediainterests  \\\n",
       "0  books: absurdistan, the republic, of mice and ...   \n",
       "1  books: to kill a mockingbird, lord of the ring...   \n",
       "2  i like some tv. i love summer heights high and...   \n",
       "3  i'm constantly reading, i read at what my frie...   \n",
       "4  books = yes. avid reader. moves = eternal suns...   \n",
       "\n",
       "                                       cantdowithout  \\\n",
       "0                  food. water. cell phone. shelter.   \n",
       "1  like everyone else, i love my friends and fami...   \n",
       "2  music, my guitar contrast good food my bike my...   \n",
       "3            family friends food women music reading   \n",
       "4  guitar - even if i don't play it all the time,...   \n",
       "\n",
       "                                     oftenthinkabout  \\\n",
       "0                        duality and humorous things   \n",
       "1  what my contribution to the world is going to ...   \n",
       "2                                         you should   \n",
       "3  snowboarding, food, women, goofy nerd stuff, a...   \n",
       "4  a little bit of everything. but mostly social ...   \n",
       "\n",
       "                                         usualfriday  \\\n",
       "0  trying to find someone to hang out with. i am ...   \n",
       "1                               out with my friends!   \n",
       "2                                     send a message   \n",
       "3  having dinner and drinks with friends and/or w...   \n",
       "4  hanging out with a small group of friends--sta...   \n",
       "\n",
       "                                              secret  \\\n",
       "0  i am new to california and looking for someone...   \n",
       "1  i cried on my first day at school because a bi...   \n",
       "2                                        and say hi.   \n",
       "3  i used to wish for a jetpack when blowing out ...   \n",
       "4  i'm picky when it comes to dating. i know what...   \n",
       "\n",
       "                                         messagemeif  \\\n",
       "0  you want to be swept off your feet! you are ti...   \n",
       "1                                    you're awesome.   \n",
       "2                             you can rock the bells   \n",
       "3  you are a complex woman with healthy self-este...   \n",
       "4  if you know who you are, who you want, where y...   \n",
       "\n",
       "                                       think_revised  stopwords  \\\n",
       "0                            duality humorous things          1   \n",
       "1  contribution world going and/or be. what's bre...         11   \n",
       "2                                                NaN          2   \n",
       "3  snowboarding, food, women, goofy nerd stuff, a...          0   \n",
       "4  little bit everything. mostly social influence...         16   \n",
       "\n",
       "                                         thinknopunc  char_count  \\\n",
       "0                            duality humorous things          23   \n",
       "1  contribution world going andor be whats breakf...          64   \n",
       "2                                                NaN           0   \n",
       "3  snowboarding food women goofy nerd stuff archi...          59   \n",
       "4  little bit everything mostly social influences...         170   \n",
       "\n",
       "                                          thinkredux  \\\n",
       "0                            duality humorous things   \n",
       "1  contribution world going andor be whats breakf...   \n",
       "2                                                NaN   \n",
       "3  snowboarding food women goofy nerd stuff archi...   \n",
       "4  little bit everything mostly social influences...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0                   ['duality', 'humorous', 'thing']   \n",
       "1  ['contribution', 'world', 'going', 'andor', 'b...   \n",
       "2                                                 []   \n",
       "3  ['snowboarding', 'food', 'woman', 'goofy', 'ne...   \n",
       "4  ['little', 'bit', 'everything', 'mostly', 'soc...   \n",
       "\n",
       "                                    think_lemmatized  \n",
       "0                   ['duality', 'humorous', 'thing']  \n",
       "1  ['contribution', 'world', 'going', 'andor', 'b...  \n",
       "2                                                 []  \n",
       "3  ['snowboarding', 'food', 'woman', 'goofy', 'ne...  \n",
       "4  ['little', 'bit', 'everything', 'mostly', 'soc...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file = 'okcupidreduxdata.csv'\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29866 entries, 0 to 29865\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Unnamed: 0             29866 non-null  int64 \n",
      " 1   age                    29866 non-null  int64 \n",
      " 2   sex                    29866 non-null  object\n",
      " 3   about me               29866 non-null  object\n",
      " 4   doingwithlife          29866 non-null  object\n",
      " 5   strengths              29866 non-null  object\n",
      " 6   whatothersnoticefirst  29866 non-null  object\n",
      " 7   mediainterests         29866 non-null  object\n",
      " 8   cantdowithout          29866 non-null  object\n",
      " 9   oftenthinkabout        29866 non-null  object\n",
      " 10  usualfriday            29866 non-null  object\n",
      " 11  secret                 29866 non-null  object\n",
      " 12  messagemeif            29866 non-null  object\n",
      " 13  think_revised          29819 non-null  object\n",
      " 14  stopwords              29866 non-null  int64 \n",
      " 15  thinknopunc            29806 non-null  object\n",
      " 16  char_count             29866 non-null  int64 \n",
      " 17  thinkredux             29806 non-null  object\n",
      " 18  text_lemmatized        29866 non-null  object\n",
      " 19  think_lemmatized       29866 non-null  object\n",
      "dtypes: int64(4), object(16)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe add in another column and look at similarities? or come up w/keywords for values ie family, fitness, and make a new list for people who have that value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         ['duality', 'humorous', 'thing']\n",
      "404      ['universe', 'a', 'booktv', 'show', 'universe'...\n",
      "947      ['relationship', 'communication', 'biz', 'trav...\n",
      "1346               ['humor', 'baseball', 'friend', 'work']\n",
      "1909     ['architectural', 'portfolio', '75', 'time', '...\n",
      "                               ...                        \n",
      "28072    ['thinking', 'u', 'think', 'spamming', 'accoun...\n",
      "28134    ['wonderful', 'life', 'is!', 'really', 'meetin...\n",
      "28605    ['fortunate', 'create', 'reality', 'give', 'ge...\n",
      "29063    ['humor', 'art', 'music', 'poetry', 'sex', 'lo...\n",
      "29732    ['accent', 'absurdity', 'adventure', 'androgyn...\n",
      "Name: think_lemmatized, Length: 87, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# testing out just one to see if this thing works\n",
    "\n",
    "df1 = df[df['think_lemmatized'].str.contains(\"humor\")]\n",
    "print(df1['think_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>about me</th>\n",
       "      <th>doingwithlife</th>\n",
       "      <th>strengths</th>\n",
       "      <th>whatothersnoticefirst</th>\n",
       "      <th>mediainterests</th>\n",
       "      <th>cantdowithout</th>\n",
       "      <th>oftenthinkabout</th>\n",
       "      <th>usualfriday</th>\n",
       "      <th>secret</th>\n",
       "      <th>messagemeif</th>\n",
       "      <th>think_revised</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>thinknopunc</th>\n",
       "      <th>char_count</th>\n",
       "      <th>thinkredux</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>think_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>about me:  i would love to think that i was so...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh. ranting about a good salt...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>books: absurdistan, the republic, of mice and ...</td>\n",
       "      <td>food. water. cell phone. shelter.</td>\n",
       "      <td>duality and humorous things</td>\n",
       "      <td>trying to find someone to hang out with. i am ...</td>\n",
       "      <td>i am new to california and looking for someone...</td>\n",
       "      <td>you want to be swept off your feet! you are ti...</td>\n",
       "      <td>duality humorous things</td>\n",
       "      <td>1</td>\n",
       "      <td>duality humorous things</td>\n",
       "      <td>23</td>\n",
       "      <td>duality humorous things</td>\n",
       "      <td>['duality', 'humorous', 'thing']</td>\n",
       "      <td>['duality', 'humorous', 'thing']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>824</td>\n",
       "      <td>25</td>\n",
       "      <td>m</td>\n",
       "      <td>recently censored so i am harder to search by ...</td>\n",
       "      <td>i work at ..., a company that makes inexpensiv...</td>\n",
       "      <td>i write comic strips that people seem to like....</td>\n",
       "      <td>- online: i can't spell - in person: i usually...</td>\n",
       "      <td>i will put a more comprehensive list up eventu...</td>\n",
       "      <td>stupidest question ... ever because i find thi...</td>\n",
       "      <td>- i have a few \"universes\" (as in book/tv show...</td>\n",
       "      <td>at the hacker dojo</td>\n",
       "      <td>- occasionally i go to bars, parties, ect., be...</td>\n",
       "      <td>in general i would greatly appreciate it if yo...</td>\n",
       "      <td>\"universes\" (as book/tv show universes) head e...</td>\n",
       "      <td>68</td>\n",
       "      <td>universes as booktv show universes head expand...</td>\n",
       "      <td>569</td>\n",
       "      <td>universes as booktv show universes head expand...</td>\n",
       "      <td>['universe', 'a', 'booktv', 'show', 'universe'...</td>\n",
       "      <td>['universe', 'a', 'booktv', 'show', 'universe'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>1908</td>\n",
       "      <td>49</td>\n",
       "      <td>m</td>\n",
       "      <td>if you take any stock in the eneagram, i'm def...</td>\n",
       "      <td>i'm an entrepreneur who runs his own team buil...</td>\n",
       "      <td>writing, solving crossword puzzles and sudokus...</td>\n",
       "      <td>my eyes or my smile, i suppose. it's hard to k...</td>\n",
       "      <td>when it comes to books and movies, i'm kind of...</td>\n",
       "      <td>in no particular order: movies books travel my...</td>\n",
       "      <td>relationships, communication, my biz, travel, ...</td>\n",
       "      <td>watching a movie, or reading a great book at h...</td>\n",
       "      <td>that i cried at the latest muppet movie. what ...</td>\n",
       "      <td>...you agree that relationships are built on c...</td>\n",
       "      <td>relationships, communication, biz, travel, pol...</td>\n",
       "      <td>53</td>\n",
       "      <td>relationships communication biz travel politic...</td>\n",
       "      <td>482</td>\n",
       "      <td>relationships communication biz travel politic...</td>\n",
       "      <td>['relationship', 'communication', 'biz', 'trav...</td>\n",
       "      <td>['relationship', 'communication', 'biz', 'trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>2805</td>\n",
       "      <td>34</td>\n",
       "      <td>m</td>\n",
       "      <td>my interests include baseball, foreign/indepen...</td>\n",
       "      <td>taking one day at a time, loving the moment, b...</td>\n",
       "      <td>-making people laugh -search engine marketing ...</td>\n",
       "      <td>my boyish charm and my midwest hospitality(eve...</td>\n",
       "      <td>y: the last man, the essential harlan ellison,...</td>\n",
       "      <td>1.time machines 2.dinosaurs 3.super intelligen...</td>\n",
       "      <td>-humor -baseball -friends -$$$ (work)</td>\n",
       "      <td>if i'm not teaching at-risk juvenile delinquen...</td>\n",
       "      <td>i read comic books, both independent or mainst...</td>\n",
       "      <td>you want to meet a someone with a sense of hum...</td>\n",
       "      <td>-humor -baseball -friends -$$$ (work)</td>\n",
       "      <td>0</td>\n",
       "      <td>humor baseball friends  work</td>\n",
       "      <td>28</td>\n",
       "      <td>humor baseball friends  work</td>\n",
       "      <td>['humor', 'baseball', 'friend', 'work']</td>\n",
       "      <td>['humor', 'baseball', 'friend', 'work']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>3984</td>\n",
       "      <td>27</td>\n",
       "      <td>m</td>\n",
       "      <td>\"a detached, observant, and indifferent man. i...</td>\n",
       "      <td>what am i doing....well, i am in pursuit of ha...</td>\n",
       "      <td>/anything related to design: tattoo, drawing, ...</td>\n",
       "      <td>is that am different, indifferent, introverted...</td>\n",
       "      <td>books: fountainhead; atlas shrugged; the giver...</td>\n",
       "      <td>ipod, laptop, iphone, brain, limbs w/ a majori...</td>\n",
       "      <td>my architectural portfolio (75% of the time); ...</td>\n",
       "      <td>everywhere and nowhere - the paradox of life.</td>\n",
       "      <td>.......eh, given that i am a secretive person,...</td>\n",
       "      <td>you feel like it.  you are intelligent, inquis...</td>\n",
       "      <td>architectural portfolio (75% time); real meani...</td>\n",
       "      <td>70</td>\n",
       "      <td>architectural portfolio 75 time real meaning l...</td>\n",
       "      <td>946</td>\n",
       "      <td>architectural portfolio 75 time real meaning l...</td>\n",
       "      <td>['architectural', 'portfolio', '75', 'time', '...</td>\n",
       "      <td>['architectural', 'portfolio', '75', 'time', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  age sex                                           about me  \\\n",
       "0              0   22   m  about me:  i would love to think that i was so...   \n",
       "404          824   25   m  recently censored so i am harder to search by ...   \n",
       "947         1908   49   m  if you take any stock in the eneagram, i'm def...   \n",
       "1346        2805   34   m  my interests include baseball, foreign/indepen...   \n",
       "1909        3984   27   m  \"a detached, observant, and indifferent man. i...   \n",
       "\n",
       "                                          doingwithlife  \\\n",
       "0     currently working as an international agent fo...   \n",
       "404   i work at ..., a company that makes inexpensiv...   \n",
       "947   i'm an entrepreneur who runs his own team buil...   \n",
       "1346  taking one day at a time, loving the moment, b...   \n",
       "1909  what am i doing....well, i am in pursuit of ha...   \n",
       "\n",
       "                                              strengths  \\\n",
       "0     making people laugh. ranting about a good salt...   \n",
       "404   i write comic strips that people seem to like....   \n",
       "947   writing, solving crossword puzzles and sudokus...   \n",
       "1346  -making people laugh -search engine marketing ...   \n",
       "1909  /anything related to design: tattoo, drawing, ...   \n",
       "\n",
       "                                  whatothersnoticefirst  \\\n",
       "0     the way i look. i am a six foot half asian, ha...   \n",
       "404   - online: i can't spell - in person: i usually...   \n",
       "947   my eyes or my smile, i suppose. it's hard to k...   \n",
       "1346  my boyish charm and my midwest hospitality(eve...   \n",
       "1909  is that am different, indifferent, introverted...   \n",
       "\n",
       "                                         mediainterests  \\\n",
       "0     books: absurdistan, the republic, of mice and ...   \n",
       "404   i will put a more comprehensive list up eventu...   \n",
       "947   when it comes to books and movies, i'm kind of...   \n",
       "1346  y: the last man, the essential harlan ellison,...   \n",
       "1909  books: fountainhead; atlas shrugged; the giver...   \n",
       "\n",
       "                                          cantdowithout  \\\n",
       "0                     food. water. cell phone. shelter.   \n",
       "404   stupidest question ... ever because i find thi...   \n",
       "947   in no particular order: movies books travel my...   \n",
       "1346  1.time machines 2.dinosaurs 3.super intelligen...   \n",
       "1909  ipod, laptop, iphone, brain, limbs w/ a majori...   \n",
       "\n",
       "                                        oftenthinkabout  \\\n",
       "0                           duality and humorous things   \n",
       "404   - i have a few \"universes\" (as in book/tv show...   \n",
       "947   relationships, communication, my biz, travel, ...   \n",
       "1346              -humor -baseball -friends -$$$ (work)   \n",
       "1909  my architectural portfolio (75% of the time); ...   \n",
       "\n",
       "                                            usualfriday  \\\n",
       "0     trying to find someone to hang out with. i am ...   \n",
       "404                                  at the hacker dojo   \n",
       "947   watching a movie, or reading a great book at h...   \n",
       "1346  if i'm not teaching at-risk juvenile delinquen...   \n",
       "1909      everywhere and nowhere - the paradox of life.   \n",
       "\n",
       "                                                 secret  \\\n",
       "0     i am new to california and looking for someone...   \n",
       "404   - occasionally i go to bars, parties, ect., be...   \n",
       "947   that i cried at the latest muppet movie. what ...   \n",
       "1346  i read comic books, both independent or mainst...   \n",
       "1909  .......eh, given that i am a secretive person,...   \n",
       "\n",
       "                                            messagemeif  \\\n",
       "0     you want to be swept off your feet! you are ti...   \n",
       "404   in general i would greatly appreciate it if yo...   \n",
       "947   ...you agree that relationships are built on c...   \n",
       "1346  you want to meet a someone with a sense of hum...   \n",
       "1909  you feel like it.  you are intelligent, inquis...   \n",
       "\n",
       "                                          think_revised  stopwords  \\\n",
       "0                               duality humorous things          1   \n",
       "404   \"universes\" (as book/tv show universes) head e...         68   \n",
       "947   relationships, communication, biz, travel, pol...         53   \n",
       "1346              -humor -baseball -friends -$$$ (work)          0   \n",
       "1909  architectural portfolio (75% time); real meani...         70   \n",
       "\n",
       "                                            thinknopunc  char_count  \\\n",
       "0                               duality humorous things          23   \n",
       "404   universes as booktv show universes head expand...         569   \n",
       "947   relationships communication biz travel politic...         482   \n",
       "1346                       humor baseball friends  work          28   \n",
       "1909  architectural portfolio 75 time real meaning l...         946   \n",
       "\n",
       "                                             thinkredux  \\\n",
       "0                               duality humorous things   \n",
       "404   universes as booktv show universes head expand...   \n",
       "947   relationships communication biz travel politic...   \n",
       "1346                       humor baseball friends  work   \n",
       "1909  architectural portfolio 75 time real meaning l...   \n",
       "\n",
       "                                        text_lemmatized  \\\n",
       "0                      ['duality', 'humorous', 'thing']   \n",
       "404   ['universe', 'a', 'booktv', 'show', 'universe'...   \n",
       "947   ['relationship', 'communication', 'biz', 'trav...   \n",
       "1346            ['humor', 'baseball', 'friend', 'work']   \n",
       "1909  ['architectural', 'portfolio', '75', 'time', '...   \n",
       "\n",
       "                                       think_lemmatized  \n",
       "0                      ['duality', 'humorous', 'thing']  \n",
       "404   ['universe', 'a', 'booktv', 'show', 'universe'...  \n",
       "947   ['relationship', 'communication', 'biz', 'trav...  \n",
       "1346            ['humor', 'baseball', 'friend', 'work']  \n",
       "1909  ['architectural', 'portfolio', '75', 'time', '...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0               87\n",
       "age                      87\n",
       "sex                      87\n",
       "about me                 87\n",
       "doingwithlife            87\n",
       "strengths                87\n",
       "whatothersnoticefirst    87\n",
       "mediainterests           87\n",
       "cantdowithout            87\n",
       "oftenthinkabout          87\n",
       "usualfriday              87\n",
       "secret                   87\n",
       "messagemeif              87\n",
       "think_revised            87\n",
       "stopwords                87\n",
       "thinknopunc              87\n",
       "char_count               87\n",
       "thinkredux               87\n",
       "text_lemmatized          87\n",
       "think_lemmatized         87\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so it works - let's come up with some actual buzzwords for values, maybe combine w/bigrams?\n",
    "\n",
    "make 3 dataframe lists \n",
    "find something to coolify it/combine it with - self awareness second column check?? or just use the info to come up \n",
    "w/better questions? see if those words also use other words - each list check for common bigrams***\n",
    "make simple landing page w/questions and connect to a typeform? can use ccard \n",
    "\n",
    "list of buzzwords - \n",
    "\n",
    "value    -    buzzwords\n",
    "\n",
    "fitness     shape, health, wellness, weight, train, gym, strength, yoga, body, nutrition, supplement, workout, exercise, martial, cardio, agile, baseball, basketball, wrestling, danc, movement, flexibility\n",
    "personal growth       improve, learn, habit, skill, mind, how, new, try, alive, grow, train, way, why, bucket, better, fun, explore, work, business, experience\n",
    "family/interpersonal      people, family, friends, relationship, communication, collaboration, competition, community, conversation, talk, hang, connect, weekend\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4        ['little', 'bit', 'everything', 'mostly', 'soc...\n",
      "9        ['build', 'empire', 'legacy', 'far', 'list', '...\n",
      "13       ['symbiosis', 'go', 'weekend', 'politics', 'tr...\n",
      "19       ['life', 'universe', 'everything', 'me', 'frie...\n",
      "22       ['littlest', 'thing', 'catapulting', 'daydream...\n",
      "                               ...                        \n",
      "29778    ['life', 'going', 'cook', 'next', 'various', '...\n",
      "29782                 ['want', 'day', 'ex', 'gym', 'work']\n",
      "29806    ['doggy', 'society', 'politics', 'religion', '...\n",
      "29824    ['anthropology', 'world', 'work', 'body', 'wor...\n",
      "29858    ['bring', 'healthy', 'loving', 'world', 'film'...\n",
      "Name: think_lemmatized, Length: 1975, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df1 = df[df['think_lemmatized'].str.contains(\"shape\") | df['think_lemmatized'].str.contains(\"health\") \n",
    "         | df['think_lemmatized'].str.contains(\"wellness\") | df['think_lemmatized'].str.contains(\"weight\")\n",
    "         | df['think_lemmatized'].str.contains(\"train\") | df['think_lemmatized'].str.contains(\"gym\")\n",
    "         | df['think_lemmatized'].str.contains(\"strength\") | df['think_lemmatized'].str.contains(\"yoga\")\n",
    "         | df['think_lemmatized'].str.contains(\"body\") | df['think_lemmatized'].str.contains(\"nutrition\")\n",
    "         | df['think_lemmatized'].str.contains(\"supplement\") | df['think_lemmatized'].str.contains(\"workout\")\n",
    "         | df['think_lemmatized'].str.contains(\"exercise\") | df['think_lemmatized'].str.contains(\"martial\")\n",
    "         | df['think_lemmatized'].str.contains(\"cardio\") | df['think_lemmatized'].str.contains(\"agile\")\n",
    "         | df['think_lemmatized'].str.contains(\"baseball\") | df['think_lemmatized'].str.contains(\"basketball\")\n",
    "         | df['think_lemmatized'].str.contains(\"wrestl\") | df['think_lemmatized'].str.contains(\"danc\")\n",
    "         | df['think_lemmatized'].str.contains(\"movement\") | df['think_lemmatized'].str.contains(\"flex\")\n",
    "        \n",
    "        \n",
    "        ]\n",
    "\n",
    "print(df1['think_lemmatized'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4        ['little', 'bit', 'everything', 'mostly', 'soc...\n",
      "5        ['methodology', 'practicing', 'creative', 'ski...\n",
      "7        ['like', 'thinking', 'way', 'improve', 'thing'...\n",
      "8                   ['passion', 'searching', 'new', 'one']\n",
      "12       ['leave', 'child', 'when', 'eventually', 'them...\n",
      "                               ...                        \n",
      "29855    ['how', 'better', 'parent', 'friend', 'daughte...\n",
      "29858    ['bring', 'healthy', 'loving', 'world', 'film'...\n",
      "29859    ['web', 'new', 'social', 'medium', 'collaborat...\n",
      "29862    ['thinking', 'bus', 'tofrom', 'work', 'usually...\n",
      "29863                 ['aside', 'work', 'improve', 'home']\n",
      "Name: think_lemmatized, Length: 11922, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df2 = df[df['think_lemmatized'].str.contains(\"improve\") | df['think_lemmatized'].str.contains(\"learn\") \n",
    "         | df['think_lemmatized'].str.contains(\"habit\") | df['think_lemmatized'].str.contains(\"skill\")\n",
    "         | df['think_lemmatized'].str.contains(\"mind\") | df['think_lemmatized'].str.contains(\"how\")\n",
    "         | df['think_lemmatized'].str.contains(\"new\") | df['think_lemmatized'].str.contains(\"try\")\n",
    "         | df['think_lemmatized'].str.contains(\"alive\") | df['think_lemmatized'].str.contains(\"grow\")\n",
    "         | df['think_lemmatized'].str.contains(\"train\") | df['think_lemmatized'].str.contains(\"way\")\n",
    "         | df['think_lemmatized'].str.contains(\"why\") | df['think_lemmatized'].str.contains(\"bucket\")\n",
    "         | df['think_lemmatized'].str.contains(\"better\") | df['think_lemmatized'].str.contains(\"fun\")\n",
    "         | df['think_lemmatized'].str.contains(\"explore\") | df['think_lemmatized'].str.contains(\"work\")\n",
    "         | df['think_lemmatized'].str.contains(\"business\") | df['think_lemmatized'].str.contains(\"experience\")     \n",
    "        \n",
    "        ]\n",
    "\n",
    "\n",
    "print(df2['think_lemmatized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4        ['little', 'bit', 'everything', 'mostly', 'soc...\n",
      "12       ['leave', 'child', 'when', 'eventually', 'them...\n",
      "13       ['symbiosis', 'go', 'weekend', 'politics', 'tr...\n",
      "20       ['build', 'thingsmake', 'thing', 'better', 'co...\n",
      "23       ['im', 'going', 'travel', 'next', 'amazing', '...\n",
      "                               ...                        \n",
      "29849    ['scheming', 'love', 'coming', 'great', 'schem...\n",
      "29853    ['everything', 'hidden', 'line', 'people', 'st...\n",
      "29858    ['bring', 'healthy', 'loving', 'world', 'film'...\n",
      "29859    ['web', 'new', 'social', 'medium', 'collaborat...\n",
      "29865    ['sex', 'myself', 'people', 'amazing', 'everyt...\n",
      "Name: think_lemmatized, Length: 7052, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df3 = df[df['think_lemmatized'].str.contains(\"people\") | df['think_lemmatized'].str.contains(\"family\") \n",
    "         | df['think_lemmatized'].str.contains(\"friends\") | df['think_lemmatized'].str.contains(\"relationship\")\n",
    "         | df['think_lemmatized'].str.contains(\"communication\") | df['think_lemmatized'].str.contains(\"collaboration\")\n",
    "         | df['think_lemmatized'].str.contains(\"competition\") | df['think_lemmatized'].str.contains(\"community\")\n",
    "         | df['think_lemmatized'].str.contains(\"conversation\") | df['think_lemmatized'].str.contains(\"talk\")\n",
    "         | df['think_lemmatized'].str.contains(\"hang\") | df['think_lemmatized'].str.contains(\"connect\")\n",
    "         | df['think_lemmatized'].str.contains(\"weekend\")    \n",
    "        \n",
    "        ]\n",
    "\n",
    "\n",
    "print(df3['think_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check each list for common bigrams / mutual information scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "post_str = ' '.join(df1['thinkredux'].tolist())\n",
    "\n",
    "print (type(post_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72711\n"
     ]
    }
   ],
   "source": [
    "#let's tokenize some stuff - bag of words\n",
    "\n",
    "tokens = nltk.word_tokenize(post_str)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos = nltk.pos_tag(tokens) #tagging pos in all the tokens plus the individual words themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list 1\n",
    "\n",
    "bigram_measures1 = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder1 = BigramCollocationFinder.from_words(tokens_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_scored = finder1.score_ngrams(bigram_measures1.raw_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('lot', 'NN'), ('time', 'NN')), 0.0014303200341076315),\n",
       " ((('time', 'NN'), ('thinking', 'VBG')), 0.0013753077251034919),\n",
       " ((('im', 'NN'), ('going', 'VBG')), 0.0013065423388483174),\n",
       " ((('!', '.'), ('!', '.')), 0.0009764684848234793),\n",
       " ((('also', 'RB'), ('think', 'VBP')), 0.0006601477080496761),\n",
       " ((('spend', 'VBP'), ('lot', 'NN')), 0.0005776292445434666),\n",
       " ((('would', 'MD'), ('like', 'VB')), 0.0005776292445434666),\n",
       " ((('also', 'RB'), ('spend', 'VBP')), 0.000508863858288292),\n",
       " ((('health', 'NN'), ('care', 'NN')), 0.000508863858288292),\n",
       " ((('think', 'VBP'), ('lot', 'NN')), 0.0004951107810372571),\n",
       " ((('friends', 'NNS'), ('family', 'NN')), 0.00039883924028001264),\n",
       " ((('next', 'JJ'), ('trip', 'NN')), 0.00039883924028001264),\n",
       " ((('many', 'JJ'), ('people', 'NNS')), 0.0003850861630289777),\n",
       " ((('social', 'JJ'), ('justice', 'NN')), 0.0003850861630289777),\n",
       " ((('things', 'NNS'), ('like', 'IN')), 0.0003713330857779428),\n",
       " ((('martial', 'JJ'), ('arts', 'NNS')), 0.0003575800085269079),\n",
       " ((('family', 'NN'), ('friends', 'VBZ')), 0.00034382693127587297),\n",
       " ((('much', 'JJ'), ('time', 'NN')), 0.00034382693127587297),\n",
       " ((('world', 'NN'), ('better', 'RBR')), 0.00034382693127587297),\n",
       " ((('right', 'RB'), ('now', 'RB')), 0.00033007385402483805),\n",
       " ((('spend', 'VB'), ('lot', 'NN')), 0.00033007385402483805),\n",
       " ((('gon', 'NN'), ('na', 'TO')), 0.00031632077677380314),\n",
       " ((('make', 'VBP'), ('world', 'NN')), 0.0002888146222717333),\n",
       " ((('one', 'CD'), ('day', 'NN')), 0.0002888146222717333),\n",
       " ((('going', 'VBG'), ('eat', 'JJ')), 0.00026130846776966347),\n",
       " ((('human', 'JJ'), ('body', 'NN')), 0.00026130846776966347),\n",
       " ((('every', 'DT'), ('day', 'NN')), 0.00024755539051862855),\n",
       " ((('people', 'NNS'), ('think', 'VBP')), 0.00024755539051862855),\n",
       " ((('spend', 'NN'), ('lot', 'NN')), 0.00024755539051862855),\n",
       " ((('travel', 'NN'), ('next', 'JJ')), 0.00024755539051862855),\n",
       " ((('want', 'VBP'), ('travel', 'NN')), 0.00024755539051862855),\n",
       " ((('better', 'RBR'), ('place', 'NN')), 0.00023380231326759364),\n",
       " ((('id', 'VBP'), ('like', 'IN')), 0.00023380231326759364),\n",
       " ((('next', 'JJ'), ('meal', 'NN')), 0.00023380231326759364),\n",
       " ((('around', 'IN'), ('me', 'PRP')), 0.0002200492360165587),\n",
       " ((('can', 'MD'), ('not', 'RB')), 0.0002200492360165587),\n",
       " ((('click', 'NN'), ('here', 'RB')), 0.0002200492360165587),\n",
       " ((('meaning', 'VBG'), ('life', 'NN')), 0.0002200492360165587),\n",
       " ((('better', 'RBR'), ('person', 'NN')), 0.00020629615876552378),\n",
       " ((('family', 'NN'), ('friends', 'NNS')), 0.00020629615876552378),\n",
       " ((('im', 'VBP'), ('going', 'VBG')), 0.00020629615876552378),\n",
       " ((('many', 'JJ'), ('things', 'NNS')), 0.00020629615876552378),\n",
       " ((('things', 'NNS'), ('want', 'VBP')), 0.00020629615876552378),\n",
       " ((('10', 'CD'), ('years', 'NNS')), 0.00019254308151448886),\n",
       " ((('around', 'IN'), ('world', 'NN')), 0.00019254308151448886),\n",
       " ((('go', 'VBP'), ('next', 'JJ')), 0.00019254308151448886),\n",
       " ((('like', 'IN'), ('think', 'VBP')), 0.00019254308151448886),\n",
       " ((('mental', 'JJ'), ('health', 'NN')), 0.00019254308151448886),\n",
       " ((('next', 'JJ'), ('adventure', 'NN')), 0.00019254308151448886),\n",
       " ((('san', 'JJ'), ('francisco', 'JJ')), 0.00019254308151448886)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "fitness_scored[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move on to info scores for d1\n",
    "\n",
    "# mutual info scores\n",
    "\n",
    "fitpmi_finder = BigramCollocationFinder.from_words(tokens_pos)\n",
    "\n",
    "fitpmi_finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_pmi_scored = fitpmi_finder.score_ngrams(bigram_measures1.pmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('hallelujah', 'NN'), ('hallelujah', 'NN')), 11.857104267437297),\n",
       " ((('gon', 'NN'), ('na', 'TO')), 11.14988601666514),\n",
       " ((('san', 'JJ'), ('francisco', 'JJ')), 10.962887501863886),\n",
       " ((('martial', 'JJ'), ('arts', 'NNS')), 10.925513231200453),\n",
       " ((('zombie', 'NN'), ('apocalypse', 'NN')), 10.827957921777779),\n",
       " ((('san', 'JJ'), ('francisco', 'NN')), 10.827957921777777),\n",
       " ((('video', 'NN'), ('games', 'NNS')), 10.49297367406497),\n",
       " ((('click', 'NN'), ('here', 'RB')), 10.018029056056346),\n",
       " ((('particular', 'JJ'), ('order', 'NN')), 9.895072117636316),\n",
       " ((('can', 'MD'), ('not', 'RB')), 9.32016328157908),\n",
       " ((('trying', 'VBG'), ('figure', 'NN')), 8.901958503221556),\n",
       " ((('right', 'RB'), ('now', 'RB')), 8.857564383863101),\n",
       " ((('cook', 'NN'), ('dinner', 'NN')), 8.740495080527438),\n",
       " ((('years', 'NNS'), ('ago', 'RB')), 8.62397375592179),\n",
       " ((('10', 'CD'), ('years', 'NNS')), 8.602440594372148),\n",
       " ((('social', 'JJ'), ('justice', 'NN')), 8.27424635504106),\n",
       " ((('human', 'JJ'), ('beings', 'NNS')), 8.259483543845192),\n",
       " ((('human', 'JJ'), ('condition', 'NN')), 8.259483543845192),\n",
       " ((('present', 'JJ'), ('moment', 'NN')), 8.247163055271852),\n",
       " ((('social', 'JJ'), ('movements', 'NNS')), 7.926323051620754),\n",
       " ((('spend', 'NN'), ('lot', 'NN')), 7.5120536145181855),\n",
       " ((('everyone', 'NN'), ('else', 'RB')), 7.432552317211448),\n",
       " ((('spend', 'VB'), ('lot', 'NN')), 7.407716954703449),\n",
       " ((('go', 'VB'), ('gym', 'JJ')), 7.400016589268297),\n",
       " ((('would', 'MD'), ('like', 'VB')), 7.3692290855076585),\n",
       " ((('spend', 'VBP'), ('lot', 'NN')), 7.355934412600906),\n",
       " ((('family', 'NN'), ('friends', 'VBZ')), 7.336489595430619),\n",
       " ((('mental', 'JJ'), ('health', 'NN')), 7.284815596751249),\n",
       " ((('lately', 'RB'), ('ive', 'JJ')), 7.243897387351016),\n",
       " ((('even', 'RB'), ('though', 'IN')), 7.237540990964515),\n",
       " ((('going', 'VBG'), ('eat', 'JJ')), 7.032037894782459),\n",
       " ((('next', 'JJ'), ('meal', 'NN')), 7.0013346660153974),\n",
       " ((('health', 'NN'), ('care', 'NN')), 6.949948446156391),\n",
       " ((('also', 'RB'), ('spend', 'VBP')), 6.859618042631078),\n",
       " ((('would', 'MD'), ('look', 'VB')), 6.821007131601521),\n",
       " ((('someone', 'NN'), ('else', 'RB')), 6.808700803096697),\n",
       " ((('id', 'VBP'), ('like', 'IN')), 6.786008939776126),\n",
       " ((('better', 'RBR'), ('place', 'NN')), 6.649571341587272),\n",
       " ((('look', 'VB'), ('like', 'IN')), 6.511614671777783),\n",
       " ((('meaning', 'VBG'), ('life', 'NN')), 6.429214229839584),\n",
       " ((('spend', 'JJ'), ('time', 'NN')), 6.412920422498933),\n",
       " ((('every', 'DT'), ('day', 'NN')), 6.394998514501674),\n",
       " ((('next', 'JJ'), ('vacation', 'NN')), 6.383357108066276),\n",
       " ((('pretty', 'RB'), ('much', 'JJ')), 6.368526303140481),\n",
       " ((('next', 'JJ'), ('trip', 'NN')), 6.3344475075853275),\n",
       " ((('amount', 'NN'), ('time', 'NN')), 6.3267637787492195),\n",
       " ((('friends', 'NNS'), ('family', 'NN')), 6.284099426214709),\n",
       " ((('eat', 'JJ'), ('next', 'JJ')), 6.225815831079794),\n",
       " ((('think', 'NN'), ('lot', 'NN')), 6.221538472262287),\n",
       " ((('body', 'NN'), ('language', 'NN')), 6.1697464390259835)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_pmi_scored [:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#do everything for d2\n",
    "\n",
    "post_str2 = ' '.join(df2['thinkredux'].tolist())\n",
    "\n",
    "print (type(post_str2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291327\n"
     ]
    }
   ],
   "source": [
    "#let's tokenize some stuff - bag of words\n",
    "\n",
    "tokens2 = nltk.word_tokenize(post_str2)\n",
    "print(len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos2 = nltk.pos_tag(tokens2) #tagging pos in all the tokens plus the individual words themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures2 = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder2 = BigramCollocationFinder.from_words(tokens_pos2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_scored = finder2.score_ngrams(bigram_measures2.raw_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('time', 'NN'), ('thinking', 'VBG')), 0.0022586303363574265),\n",
       " ((('lot', 'NN'), ('time', 'NN')), 0.002018350513340679),\n",
       " ((('im', 'NN'), ('going', 'VBG')), 0.0014382463692002457),\n",
       " ((('!', '.'), ('!', '.')), 0.001400488111297614),\n",
       " ((('world', 'NN'), ('better', 'RBR')), 0.000892467914062205),\n",
       " ((('also', 'RB'), ('think', 'VBP')), 0.0008581422250598125),\n",
       " ((('better', 'RBR'), ('place', 'NN')), 0.0007380023135514388),\n",
       " ((('would', 'MD'), ('like', 'VB')), 0.0006693509355466538),\n",
       " ((('make', 'VBP'), ('world', 'NN')), 0.0006521880910454575),\n",
       " ((('spend', 'VB'), ('lot', 'NN')), 0.0006521880910454575),\n",
       " ((('spend', 'VBP'), ('lot', 'NN')), 0.0006281601087437828),\n",
       " ((('things', 'NNS'), ('work', 'VBP')), 0.0005595087307389978),\n",
       " ((('think', 'VBP'), ('lot', 'NN')), 0.0005595087307389978),\n",
       " ((('friends', 'NNS'), ('family', 'NN')), 0.000535480748437323),\n",
       " ((('better', 'RBR'), ('person', 'NN')), 0.0005011550594349305),\n",
       " ((('also', 'RB'), ('spend', 'VBP')), 0.0004702619393327773),\n",
       " ((('right', 'RB'), ('now', 'RB')), 0.00045653166373182025),\n",
       " ((('spend', 'NN'), ('lot', 'NN')), 0.00042907111252990624),\n",
       " ((('much', 'JJ'), ('time', 'NN')), 0.000425638543629667),\n",
       " ((('things', 'NNS'), ('like', 'IN')), 0.00039474542352751374),\n",
       " ((('around', 'IN'), ('me', 'PRP')), 0.0003913128546272745),\n",
       " ((('new', 'JJ'), ('things', 'NNS')), 0.00038788028572703525),\n",
       " ((('id', 'VBP'), ('like', 'IN')), 0.0003775825790263175),\n",
       " ((('next', 'JJ'), ('vacation', 'NN')), 0.000356987165624882),\n",
       " ((('family', 'NN'), ('friends', 'VBZ')), 0.00035355459672464274),\n",
       " ((('next', 'JJ'), ('trip', 'NN')), 0.0003501220278244035),\n",
       " ((('always', 'RB'), ('thinking', 'VBG')), 0.000343256890023925),\n",
       " ((('things', 'NNS'), ('want', 'VBP')), 0.00033982432112368577),\n",
       " ((('gon', 'NN'), ('na', 'TO')), 0.0003329591833232073),\n",
       " ((('many', 'JJ'), ('people', 'NNS')), 0.00032952661442296803),\n",
       " ((('next', 'JJ'), ('adventure', 'NN')), 0.00032609404552272873),\n",
       " ((('trying', 'VBG'), ('figure', 'NN')), 0.00032609404552272873),\n",
       " ((('make', 'VBP'), ('better', 'JJR')), 0.0003226614766224895),\n",
       " ((('meaning', 'VBG'), ('life', 'NN')), 0.0003089312010215325),\n",
       " ((('better', 'JJR'), ('person', 'NN')), 0.00030549863212129326),\n",
       " ((('im', 'NN'), ('always', 'RB')), 0.0002986334943208148),\n",
       " ((('spend', 'JJ'), ('time', 'NN')), 0.0002986334943208148),\n",
       " ((('one', 'CD'), ('day', 'NN')), 0.00029176835652033623),\n",
       " ((('people', 'NNS'), ('think', 'VBP')), 0.00029176835652033623),\n",
       " ((('things', 'NNS'), ('do', 'VBP')), 0.00029176835652033623),\n",
       " ((('want', 'VBP'), ('travel', 'NN')), 0.000288335787620097),\n",
       " ((('ways', 'NNS'), ('make', 'VBP')), 0.00028490321871985774),\n",
       " ((('every', 'DT'), ('day', 'NN')), 0.0002814706498196185),\n",
       " ((('going', 'VBG'), ('next', 'JJ')), 0.0002814706498196185),\n",
       " ((('social', 'JJ'), ('justice', 'NN')), 0.00027117294311890076),\n",
       " ((('make', 'VBP'), ('life', 'NN')), 0.0002677403742186615),\n",
       " ((('life', 'NN'), ('better', 'RBR')), 0.0002643078053184223),\n",
       " ((('way', 'NN'), ('are', 'VBP')), 0.0002643078053184223),\n",
       " ((('family', 'NN'), ('friends', 'NNS')), 0.00025744266751794373),\n",
       " ((('make', 'VBP'), ('things', 'NNS')), 0.00025744266751794373)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_scored[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutual info scores\n",
    "\n",
    "pgpmi_finder = BigramCollocationFinder.from_words(tokens_pos2)\n",
    "\n",
    "pgpmi_finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_pmi_scored = pgpmi_finder.score_ngrams(bigram_measures2.pmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('hallelujah', 'NN'), ('hallelujah', 'NN')), 13.859498140937106),\n",
       " ((('golden', 'JJ'), ('gate', 'NN')), 13.508423700390226),\n",
       " ((('ice', 'NN'), ('cream', 'NN')), 12.854129178560713),\n",
       " ((('blah', 'NN'), ('blah', 'NN')), 12.830351795277588),\n",
       " ((('martial', 'JJ'), ('arts', 'NNS')), 12.489314877442522),\n",
       " ((('artificial', 'JJ'), ('intelligence', 'NN')), 12.04375543338678),\n",
       " ((('global', 'JJ'), ('warming', 'VBG')), 11.684131054426546),\n",
       " ((('ok', 'JJ'), ('cupid', 'JJ')), 11.660426793835274),\n",
       " ((('black', 'JJ'), ('holes', 'NNS')), 11.556429073301057),\n",
       " ((('san', 'NN'), ('francisco', 'NN')), 11.409465220302057),\n",
       " ((('san', 'JJ'), ('francisco', 'NN')), 11.164981210840041),\n",
       " ((('quite', 'RB'), ('bit', 'JJ')), 11.096997454663763),\n",
       " ((('zombie', 'NN'), ('apocalypse', 'NN')), 11.022996873219984),\n",
       " ((('san', 'JJ'), ('francisco', 'JJ')), 11.020422929556158),\n",
       " ((('hot', 'JJ'), ('springs', 'NNS')), 11.009940720394537),\n",
       " ((('gon', 'NN'), ('na', 'TO')), 10.933111369702793),\n",
       " ((('gon', 'VBG'), ('na', 'TO')), 10.93311136970279),\n",
       " ((('bay', 'JJ'), ('area', 'NN')), 10.931464222338606),\n",
       " ((('bay', 'VBP'), ('area', 'NN')), 10.861842081405404),\n",
       " ((('pursuit', 'NN'), ('happiness', 'NN')), 10.851823325580414),\n",
       " ((('daily', 'JJ'), ('basis', 'NN')), 10.753730513674675),\n",
       " ((('string', 'VBG'), ('theory', 'NN')), 10.67789793006168),\n",
       " ((('real', 'JJ'), ('estate', 'NN')), 10.652434003081744),\n",
       " ((('renewable', 'JJ'), ('energy', 'NN')), 10.567317389443794),\n",
       " ((('fair', 'JJ'), ('amount', 'NN')), 10.508423700390226),\n",
       " ((('todo', 'JJ'), ('list', 'NN')), 10.453975916367849),\n",
       " ((('bucket', 'NN'), ('list', 'NN')), 10.44630198848243),\n",
       " ((('online', 'NN'), ('dating', 'VBG')), 10.406885673928162),\n",
       " ((('quantum', 'NN'), ('physics', 'NNS')), 10.344339577536974),\n",
       " ((('rock', 'NN'), ('climbing', 'VBG')), 10.288867931223315),\n",
       " ((('paying', 'VBG'), ('attention', 'NN')), 10.242386806394908),\n",
       " ((('got', 'VBD'), ('ta', 'JJ')), 10.235902772824709),\n",
       " ((('self', 'PRP'), ('improvement', 'NN')), 10.20577231545985),\n",
       " ((('tv', 'NN'), ('shows', 'NNS')), 10.105986237891017),\n",
       " ((('cup', 'NN'), ('coffee', 'NN')), 10.067229986491457),\n",
       " ((('video', 'JJ'), ('game', 'NN')), 10.051378674589806),\n",
       " ((('video', 'NN'), ('games', 'NNS')), 9.933514864332992),\n",
       " ((('burning', 'VBG'), ('man', 'NN')), 9.893168191321577),\n",
       " ((('problem', 'NN'), ('solving', 'VBG')), 9.825850403042649),\n",
       " ((('loved', 'VBN'), ('ones', 'NNS')), 9.778599056753338),\n",
       " ((('answer', 'JJR'), ('question', 'NN')), 9.77030441160765),\n",
       " ((('bay', 'NN'), ('area', 'NN')), 9.76919279343973),\n",
       " ((('giving', 'VBG'), ('back', 'RP')), 9.696952669860389),\n",
       " ((('grad', 'JJ'), ('school', 'NN')), 9.645995925539772),\n",
       " ((('public', 'JJ'), ('policy', 'NN')), 9.584323814749482),\n",
       " ((('self', 'NN'), ('improvement', 'NN')), 9.464702322960907),\n",
       " ((('long', 'JJ'), ('term', 'NN')), 9.42910124772882),\n",
       " ((('necessarily', 'RB'), ('order', 'NN')), 9.418417170486407),\n",
       " ((('loved', 'VBD'), ('ones', 'NNS')), 9.33401728186048),\n",
       " ((('pretty', 'JJ'), ('sure', 'JJ')), 9.321289408915689)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_pmi_scored [:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#do this again for list 3 - so far the pmi scores have differed pretty well\n",
    "post_str3 = ' '.join(df3['thinkredux'].tolist())\n",
    "\n",
    "print (type(post_str3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191503\n"
     ]
    }
   ],
   "source": [
    "#let's tokenize some stuff - bag of words\n",
    "\n",
    "tokens3 = nltk.word_tokenize(post_str3)\n",
    "print(len(tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos3 = nltk.pos_tag(tokens3) #tagging pos in all the tokens plus the individual words themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures3 = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder3 = BigramCollocationFinder.from_words(tokens_pos3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_scored = finder3.score_ngrams(bigram_measures3.raw_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('time', 'NN'), ('thinking', 'VBG')), 0.0019581938664146254),\n",
       " ((('lot', 'NN'), ('time', 'NN')), 0.0016918795005822363),\n",
       " ((('friends', 'NNS'), ('family', 'NN')), 0.0014621180869229203),\n",
       " ((('im', 'NN'), ('going', 'VBG')), 0.0013472373800932622),\n",
       " ((('!', '.'), ('!', '.')), 0.0011070322658130682),\n",
       " ((('family', 'NN'), ('friends', 'VBZ')), 0.0009555986068103372),\n",
       " ((('also', 'RB'), ('think', 'VBP')), 0.0007884993968762891),\n",
       " ((('many', 'JJ'), ('people', 'NNS')), 0.0007153934925301431),\n",
       " ((('family', 'NN'), ('friends', 'NNS')), 0.0006579531391153141),\n",
       " ((('would', 'MD'), ('like', 'VB')), 0.0006422875881839971),\n",
       " ((('people', 'NNS'), ('think', 'VBP')), 0.0006214001869422411),\n",
       " ((('people', 'NNS'), ('love', 'VBP')), 0.0006109564863213631),\n",
       " ((('people', 'NNS'), ('life', 'NN')), 0.0006005127857004851),\n",
       " ((('spend', 'VBP'), ('lot', 'NN')), 0.0005744035341482901),\n",
       " ((('spend', 'VB'), ('lot', 'NN')), 0.0005639598335274121),\n",
       " ((('think', 'VBP'), ('lot', 'NN')), 0.0005274068813543391),\n",
       " ((('makes', 'VBZ'), ('people', 'NNS')), 0.0005012976298021441),\n",
       " ((('world', 'NN'), ('better', 'RBR')), 0.0004804102285603881),\n",
       " ((('people', 'NNS'), ('things', 'NNS')), 0.0004490791266977541),\n",
       " ((('life', 'NN'), ('people', 'NNS')), 0.0004438572763873151),\n",
       " ((('people', 'NNS'), ('around', 'IN')), 0.0004438572763873151),\n",
       " ((('also', 'RB'), ('spend', 'VBP')), 0.0004334135757664371),\n",
       " ((('around', 'IN'), ('me', 'PRP')), 0.0004334135757664371),\n",
       " ((('things', 'NNS'), ('like', 'IN')), 0.0004125261745246811),\n",
       " ((('next', 'JJ'), ('vacation', 'NN')), 0.0003968606235933641),\n",
       " ((('change', 'NN'), ('world', 'NN')), 0.0003759732223516081),\n",
       " ((('better', 'RBR'), ('place', 'NN')), 0.0003655295217307301),\n",
       " ((('things', 'NNS'), ('do', 'VBP')), 0.0003603076714202911),\n",
       " ((('spend', 'NN'), ('lot', 'NN')), 0.0003550858211098521),\n",
       " ((('make', 'VBP'), ('world', 'NN')), 0.0003498639707994131),\n",
       " ((('things', 'NNS'), ('people', 'NNS')), 0.0003498639707994131),\n",
       " ((('people', 'NNS'), ('care', 'VBP')), 0.0003446421204889741),\n",
       " ((('gon', 'NN'), ('na', 'TO')), 0.0003394202701785351),\n",
       " ((('make', 'VBP'), ('people', 'NNS')), 0.00033419841986809603),\n",
       " ((('next', 'JJ'), ('trip', 'NN')), 0.00033419841986809603),\n",
       " ((('things', 'NNS'), ('work', 'VBP')), 0.00032897656955765704),\n",
       " ((('right', 'RB'), ('now', 'RB')), 0.00032375471924721804),\n",
       " ((('id', 'VBP'), ('like', 'IN')), 0.00031853286893677904),\n",
       " ((('much', 'JJ'), ('time', 'NN')), 0.00031331101862634004),\n",
       " ((('way', 'NN'), ('are', 'VBP')), 0.00030808916831590104),\n",
       " ((('like', 'IN'), ('think', 'VBP')), 0.00030286731800546204),\n",
       " ((('one', 'CD'), ('another', 'DT')), 0.00029242361738458404),\n",
       " ((('one', 'CD'), ('day', 'NN')), 0.00028720176707414504),\n",
       " ((('friends', 'VBZ'), ('family', 'NN')), 0.00028197991676370604),\n",
       " ((('social', 'JJ'), ('justice', 'NN')), 0.00028197991676370604),\n",
       " ((('new', 'JJ'), ('people', 'NNS')), 0.00027675806645326704),\n",
       " ((('people', 'NNS'), ('do', 'VBP')), 0.00027675806645326704),\n",
       " ((('spend', 'JJ'), ('time', 'NN')), 0.00027153621614282804),\n",
       " ((('better', 'RBR'), ('person', 'NN')), 0.00026631436583238904),\n",
       " ((('meaning', 'VBG'), ('life', 'NN')), 0.00026631436583238904)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_scored [:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutual info scores\n",
    "\n",
    "peoplepmi_finder = BigramCollocationFinder.from_words(tokens_pos3)\n",
    "\n",
    "peoplepmi_finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "peoplepmi_scored = peoplepmi_finder.score_ngrams(bigram_measures3.pmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('ted', 'VBD'), ('talks', 'NNS')), 13.409503943626781),\n",
       " ((('hallelujah', 'NN'), ('hallelujah', 'NN')), 13.25422571814887),\n",
       " ((('ice', 'NN'), ('cream', 'NN')), 12.771906744323738),\n",
       " ((('zombie', 'NN'), ('apocalypse', 'NN')), 11.177326241040983),\n",
       " ((('san', 'JJ'), ('francisco', 'JJ')), 11.140884245072924),\n",
       " ((('bay', 'VBP'), ('area', 'NN')), 11.132994973498631),\n",
       " ((('bay', 'JJ'), ('area', 'NN')), 11.093895153809695),\n",
       " ((('san', 'JJ'), ('francisco', 'NN')), 11.074962852127216),\n",
       " ((('gon', 'NN'), ('na', 'TO')), 11.001419204989698),\n",
       " ((('self', 'PRP'), ('improvement', 'NN')), 10.89165563876416),\n",
       " ((('quantum', 'NN'), ('physics', 'NNS')), 10.643728125320576),\n",
       " ((('tv', 'NN'), ('shows', 'NNS')), 10.588586571128111),\n",
       " ((('fair', 'JJ'), ('amount', 'NN')), 10.585075508210233),\n",
       " ((('online', 'NN'), ('dating', 'VBG')), 10.49607050215149),\n",
       " ((('got', 'VBD'), ('ta', 'JJ')), 10.458787107053633),\n",
       " ((('string', 'VBG'), ('theory', 'NN')), 10.411554683191541),\n",
       " ((('bay', 'NN'), ('area', 'NN')), 10.365441059499005),\n",
       " ((('video', 'NN'), ('games', 'NNS')), 10.210979175444498),\n",
       " ((('burning', 'VBG'), ('man', 'NN')), 9.805219489753664),\n",
       " ((('answer', 'JJR'), ('question', 'NN')), 9.72173063732185),\n",
       " ((('grad', 'JJ'), ('school', 'NN')), 9.719551339262601),\n",
       " ((('loved', 'VBD'), ('ones', 'NNS')), 9.494439416572561),\n",
       " ((('long', 'JJ'), ('term', 'NN')), 9.464478219131488),\n",
       " ((('can', 'MD'), ('not', 'RB')), 9.441389912732305),\n",
       " ((('road', 'NN'), ('trips', 'NNS')), 9.441098958805556),\n",
       " ((('grow', 'VB'), ('up', 'RP')), 9.34788046181764),\n",
       " ((('particular', 'JJ'), ('order', 'NN')), 9.326763512618845),\n",
       " ((('whats', 'NNS'), ('dinner', 'VBP')), 9.197173375919464),\n",
       " ((('personal', 'JJ'), ('growth', 'NN')), 9.163811434566504),\n",
       " ((('come', 'VBP'), ('from', 'IN')), 9.156122000091084),\n",
       " ((('current', 'JJ'), ('events', 'NNS')), 9.132244898073145),\n",
       " ((('positive', 'JJ'), ('impact', 'NN')), 8.9603119994311),\n",
       " ((('care', 'VBP'), ('about', 'IN')), 8.913226756001658),\n",
       " ((('human', 'JJ'), ('condition', 'NN')), 8.878122483110465),\n",
       " ((('someone', 'NN'), ('elses', 'VBZ')), 8.86015681398358),\n",
       " ((('years', 'NNS'), ('ago', 'RB')), 8.77567521413965),\n",
       " ((('etc', 'FW'), ('etc', 'FW')), 8.736757181348533),\n",
       " ((('right', 'RB'), ('now', 'RB')), 8.69103384009893),\n",
       " ((('big', 'JJ'), ('picture', 'NN')), 8.683549956596957),\n",
       " ((('cook', 'NN'), ('dinner', 'NN')), 8.642851839489431),\n",
       " ((('travel', 'NN'), ('destination', 'NN')), 8.472866004624205),\n",
       " ((('wish', 'NN'), ('could', 'MD')), 8.468084207363889),\n",
       " ((('list', 'NN'), ('goes', 'VBZ')), 8.43717681309792),\n",
       " ((('social', 'JJ'), ('justice', 'NN')), 8.416389239747932),\n",
       " ((('trying', 'VBG'), ('figure', 'NN')), 8.371229739688964),\n",
       " ((('10', 'CD'), ('years', 'NNS')), 8.344883643546252),\n",
       " ((('human', 'JJ'), ('beings', 'NNS')), 8.332688346575946),\n",
       " ((('climate', 'NN'), ('change', 'NN')), 8.264862865416653),\n",
       " ((('lucky', 'JJ'), ('am', 'VBP')), 8.238461584410096),\n",
       " ((('new', 'JJ'), ('york', 'NN')), 8.11237923973999)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peoplepmi_scored [:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seeing both similarities and differencees btw the 3 priority groups:\n",
    "\n",
    "## summary of differences\n",
    "\n",
    "\n",
    "## summary of differences\n",
    "\n",
    "# fit\n",
    "\n",
    "present moment\n",
    "\n",
    "social movements\n",
    "\n",
    "body language\n",
    "\n",
    "\n",
    "# pg\n",
    "\n",
    "golden gate\n",
    "\n",
    "blah blah\n",
    "\n",
    "artificial intelligence\n",
    "\n",
    "black holes\n",
    "\n",
    "quite bit\n",
    "\n",
    "hot springs\n",
    "\n",
    "pursuit happiness\n",
    "\n",
    "daily basis\n",
    "\n",
    "real estate\n",
    "\n",
    "renewable energy\n",
    "\n",
    "bucket list\n",
    "\n",
    "rock climbing\n",
    "\n",
    "problem solving\n",
    "\n",
    "\n",
    "# people\n",
    "\n",
    "ted talks as the top one\n",
    "\n",
    "fair amount \n",
    "\n",
    "road trips\n",
    "\n",
    "grow up\n",
    "\n",
    "personal growth\n",
    "\n",
    "current events\n",
    "\n",
    "positive impact\n",
    "\n",
    "care about\n",
    "\n",
    "someone elses\n",
    "\n",
    "etc etc\n",
    "\n",
    "big picture\n",
    "\n",
    "travel destination\n",
    "\n",
    "wish could\n",
    "\n",
    "list goes\n",
    "\n",
    "lucky am\n",
    "\n",
    "new york\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patterns of distinction\n",
    "\n",
    "personal growth -  theres a productivity/goal based theme\n",
    "people - focus on others, philosophy, big picture life stuff\n",
    "fitness - focused on small actions that can add up\n",
    "\n",
    "Let's try to tease out more of their largest core value / reason for doing snowsports or in life in general, ask questions that harder to lie about / easier to answer intuitively for most people\n",
    "\n",
    "# Question ideas:\n",
    "\n",
    "original question in dataset - what do you often think about? \n",
    "\n",
    "What is this trying to get at, and what can we use our patterns of distinction to tease out? - problem solving process, subconscious, priorities, values, philosophy, what they're curious about\n",
    "\n",
    "- what would you make better about yourself instantly if you could, even if youre already good at it?\n",
    "- when was the last time you found yourself wishing for more time?\n",
    "- philosophy question - how would you get your dog to bring you a slice of pizza?\n",
    "- whats one of your favorite topics and how would you teach someone the most basic thing on it?\n",
    "- what was the last dream you remember having when sleeping?\n",
    "- how do you approach planning / running your vacations? \n",
    "- what was your day yesterday like?\n",
    "- tell us about someone you really dislike\n",
    "- last time you felt truly alive / wanted time to stop to savor more\n",
    "- whats something that took you a long or short time to learn?\n",
    "\n",
    "- what is a metric of success/fulfillment for you personally in your life?\n",
    "- tell us about your most recent good / bad day\n",
    "- what are you looking forward to next week?\n",
    "\n",
    "Top 6 Q's:\n",
    "- what are you looking forward to next week?\n",
    "- how do you like to plan your vacations / spend your time during them? \n",
    "- tell us about someone you truly dislike and why?\n",
    "- philosophy question - how would you get your dog to bring you a slice of pizza?\n",
    "- last time you felt truly alive / wanted time to stop to savor more / or where would you go back in time in your life to experience something twice?\n",
    "- what new skill do you most want to learn right now / or what current skill do you most want to improve? and when was the last time you worked on this skill?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
